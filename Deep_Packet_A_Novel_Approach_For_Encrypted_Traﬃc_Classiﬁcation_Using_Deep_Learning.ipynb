{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Packet: A Novel Approach For Encrypted Traﬃc Classiﬁcation Using Deep Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XoQsLQrPrGwt",
        "tJJRor0mtfWk"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zGjfJ0oRXee"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYhZsBYSvpXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33d7544d-3c48-4fbc-bbb2-9e6eb656817b"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUlCc8m7mbwq"
      },
      "source": [
        "import click\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import random\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import time\r\n",
        "import functools\r\n",
        "from argparse import Namespace\r\n",
        "from pathlib import Path\r\n",
        "!pip install scapy\r\n",
        "from scapy.compat import raw\r\n",
        "from scapy.layers.inet import IP, UDP\r\n",
        "from scapy.layers.l2 import Ether\r\n",
        "from scapy.packet import Padding\r\n",
        "from scipy import sparse\r\n",
        "from scapy.layers.dns import DNS\r\n",
        "from scapy.layers.inet import TCP\r\n",
        "from scapy.packet import Padding\r\n",
        "from scapy.utils import rdpcap\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoQsLQrPrGwt"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxWJirSTrMmp"
      },
      "source": [
        "Labeling the App and Traffic different types"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH-kmNqygKDO"
      },
      "source": [
        "# for app identification\r\n",
        "PREFIX_TO_APP_ID = {\r\n",
        "    # AIM chat\r\n",
        "    'aim_chat_3a': 0,\r\n",
        "    'aim_chat_3b': 0,\r\n",
        "    'aimchat1': 0,\r\n",
        "    'aimchat2': 0,\r\n",
        "    # Email\r\n",
        "    'email1a': 1,\r\n",
        "    'email1b': 1,\r\n",
        "    'email2a': 1,\r\n",
        "    'email2b': 1,\r\n",
        "    # Facebook\r\n",
        "    'facebook_audio1a': 2,\r\n",
        "    'facebook_audio1b': 2,\r\n",
        "    'facebook_audio2a': 2,\r\n",
        "    'facebook_audio2b': 2,\r\n",
        "    'facebook_audio3': 2,\r\n",
        "    'facebook_audio4': 2,\r\n",
        "    'facebook_chat_4a': 2,\r\n",
        "    'facebook_chat_4b': 2,\r\n",
        "    'facebook_video1a': 2,\r\n",
        "    'facebook_video1b': 2,\r\n",
        "    'facebook_video2a': 2,\r\n",
        "    'facebook_video2b': 2,\r\n",
        "    'facebookchat1': 2,\r\n",
        "    'facebookchat2': 2,\r\n",
        "    'facebookchat3': 2,\r\n",
        "    # FTPS\r\n",
        "    'ftps_down_1a': 3,\r\n",
        "    'ftps_down_1b': 3,\r\n",
        "    'ftps_up_2a': 3,\r\n",
        "    'ftps_up_2b': 3,\r\n",
        "    # Gmail\r\n",
        "    'gmailchat1': 4,\r\n",
        "    'gmailchat2': 4,\r\n",
        "    'gmailchat3': 4,\r\n",
        "    # Hangouts\r\n",
        "    'hangout_chat_4b': 5,\r\n",
        "    'hangouts_audio1a': 5,\r\n",
        "    'hangouts_audio1b': 5,\r\n",
        "    'hangouts_audio2a': 5,\r\n",
        "    'hangouts_audio2b': 5,\r\n",
        "    'hangouts_audio3': 5,\r\n",
        "    'hangouts_audio4': 5,\r\n",
        "    'hangouts_chat_4a': 5,\r\n",
        "    'hangouts_video1b': 5,\r\n",
        "    'hangouts_video2a': 5,\r\n",
        "    'hangouts_video2b': 5,\r\n",
        "    # ICQ\r\n",
        "    'icq_chat_3a': 6,\r\n",
        "    'icq_chat_3b': 6,\r\n",
        "    'icqchat1': 6,\r\n",
        "    'icqchat2': 6,\r\n",
        "    # Netflix\r\n",
        "    'netflix1': 7,\r\n",
        "    'netflix2': 7,\r\n",
        "    'netflix3': 7,\r\n",
        "    'netflix4': 7,\r\n",
        "    # SCP\r\n",
        "    'scp1': 8,\r\n",
        "    'scpdown1': 8,\r\n",
        "    'scpdown2': 8,\r\n",
        "    'scpdown3': 8,\r\n",
        "    'scpdown4': 8,\r\n",
        "    'scpdown5': 8,\r\n",
        "    'scpdown6': 8,\r\n",
        "    'scpup1': 8,\r\n",
        "    'scpup2': 8,\r\n",
        "    'scpup3': 8,\r\n",
        "    'scpup5': 8,\r\n",
        "    'scpup6': 8,\r\n",
        "    # SFTP\r\n",
        "    'sftp1': 9,\r\n",
        "    'sftp_down_3a': 9,\r\n",
        "    'sftp_down_3b': 9,\r\n",
        "    'sftp_up_2a': 9,\r\n",
        "    'sftp_up_2b': 9,\r\n",
        "    'sftpdown1': 9,\r\n",
        "    'sftpdown2': 9,\r\n",
        "    'sftpup1': 9,\r\n",
        "    # Skype\r\n",
        "    'skype_audio1a': 10,\r\n",
        "    'skype_audio1b': 10,\r\n",
        "    'skype_audio2a': 10,\r\n",
        "    'skype_audio2b': 10,\r\n",
        "    'skype_audio3': 10,\r\n",
        "    'skype_audio4': 10,\r\n",
        "    'skype_chat1a': 10,\r\n",
        "    'skype_chat1b': 10,\r\n",
        "    'skype_file1': 10,\r\n",
        "    'skype_file2': 10,\r\n",
        "    'skype_file3': 10,\r\n",
        "    'skype_file4': 10,\r\n",
        "    'skype_file5': 10,\r\n",
        "    'skype_file6': 10,\r\n",
        "    'skype_file7': 10,\r\n",
        "    'skype_file8': 10,\r\n",
        "    'skype_video1a': 10,\r\n",
        "    'skype_video1b': 10,\r\n",
        "    'skype_video2a': 10,\r\n",
        "    'skype_video2b': 10,\r\n",
        "    # Spotify\r\n",
        "    'spotify1': 11,\r\n",
        "    'spotify2': 11,\r\n",
        "    'spotify3': 11,\r\n",
        "    'spotify4': 11,\r\n",
        "    # Torrent\r\n",
        "    'torrent01': 12,\r\n",
        "    # Tor\r\n",
        "    'torfacebook': 13,\r\n",
        "    'torgoogle': 13,\r\n",
        "    'tortwitter': 13,\r\n",
        "    'torvimeo1': 13,\r\n",
        "    'torvimeo2': 13,\r\n",
        "    'torvimeo3': 13,\r\n",
        "    'toryoutube1': 13,\r\n",
        "    'toryoutube2': 13,\r\n",
        "    'toryoutube3': 13,\r\n",
        "    # Vimeo\r\n",
        "    'vimeo1': 14,\r\n",
        "    'vimeo2': 14,\r\n",
        "    'vimeo3': 14,\r\n",
        "    'vimeo4': 14,\r\n",
        "    # Voipbuster\r\n",
        "    'voipbuster1b': 15,\r\n",
        "    'voipbuster2b': 15,\r\n",
        "    'voipbuster3b': 15,\r\n",
        "    'voipbuster_4a': 15,\r\n",
        "    'voipbuster_4b': 15,\r\n",
        "    # Youtube\r\n",
        "    'youtube1': 16,\r\n",
        "    'youtube2': 16,\r\n",
        "    'youtube3': 16,\r\n",
        "    'youtube4': 16,\r\n",
        "    'youtube5': 16,\r\n",
        "    'youtube6': 16,\r\n",
        "    'youtubehtml5_1': 16,\r\n",
        "}\r\n",
        "\r\n",
        "ID_TO_APP = {\r\n",
        "    0: 'AIM Chat',\r\n",
        "    1: 'Email',\r\n",
        "    2: 'Facebook',\r\n",
        "    3: 'FTPS',\r\n",
        "    4: 'Gmail',\r\n",
        "    5: 'Hangouts',\r\n",
        "    6: 'ICQ',\r\n",
        "    7: 'Netflix',\r\n",
        "    8: 'SCP',\r\n",
        "    9: 'SFTP',\r\n",
        "    10: 'Skype',\r\n",
        "    11: 'Spotify',\r\n",
        "    12: 'Torrent',\r\n",
        "    13: 'Tor',\r\n",
        "    14: 'Vimeo',\r\n",
        "    15: 'Voipbuster',\r\n",
        "    16: 'Youtube',\r\n",
        "}\r\n",
        "\r\n",
        "# for traffic identification\r\n",
        "PREFIX_TO_TRAFFIC_ID = {\r\n",
        "    # Chat\r\n",
        "    'aim_chat_3a': 0,\r\n",
        "    'aim_chat_3b': 0,\r\n",
        "    'aimchat1': 0,\r\n",
        "    'aimchat2': 0,\r\n",
        "    'facebook_chat_4a': 0,\r\n",
        "    'facebook_chat_4b': 0,\r\n",
        "    'facebookchat1': 0,\r\n",
        "    'facebookchat2': 0,\r\n",
        "    'facebookchat3': 0,\r\n",
        "    'hangout_chat_4b': 0,\r\n",
        "    'hangouts_chat_4a': 0,\r\n",
        "    'icq_chat_3a': 0,\r\n",
        "    'icq_chat_3b': 0,\r\n",
        "    'icqchat1': 0,\r\n",
        "    'icqchat2': 0,\r\n",
        "    'skype_chat1a': 0,\r\n",
        "    'skype_chat1b': 0,\r\n",
        "    # Email\r\n",
        "    'email1a': 1,\r\n",
        "    'email1b': 1,\r\n",
        "    'email2a': 1,\r\n",
        "    'email2b': 1,\r\n",
        "    # File Transfer\r\n",
        "    'ftps_down_1a': 2,\r\n",
        "    'ftps_down_1b': 2,\r\n",
        "    'ftps_up_2a': 2,\r\n",
        "    'ftps_up_2b': 2,\r\n",
        "    'sftp1': 2,\r\n",
        "    'sftp_down_3a': 2,\r\n",
        "    'sftp_down_3b': 2,\r\n",
        "    'sftp_up_2a': 2,\r\n",
        "    'sftp_up_2b': 2,\r\n",
        "    'sftpdown1': 2,\r\n",
        "    'sftpdown2': 2,\r\n",
        "    'sftpup1': 2,\r\n",
        "    'skype_file1': 2,\r\n",
        "    'skype_file2': 2,\r\n",
        "    'skype_file3': 2,\r\n",
        "    'skype_file4': 2,\r\n",
        "    'skype_file5': 2,\r\n",
        "    'skype_file6': 2,\r\n",
        "    'skype_file7': 2,\r\n",
        "    'skype_file8': 2,\r\n",
        "    # Streaming\r\n",
        "    'vimeo1': 3,\r\n",
        "    'vimeo2': 3,\r\n",
        "    'vimeo3': 3,\r\n",
        "    'vimeo4': 3,\r\n",
        "    'youtube1': 3,\r\n",
        "    'youtube2': 3,\r\n",
        "    'youtube3': 3,\r\n",
        "    'youtube4': 3,\r\n",
        "    'youtube5': 3,\r\n",
        "    'youtube6': 3,\r\n",
        "    'youtubehtml5_1': 3,\r\n",
        "    # Torrent\r\n",
        "    'torrent01': 4,\r\n",
        "    # VoIP\r\n",
        "    'facebook_audio1a': 5,\r\n",
        "    'facebook_audio1b': 5,\r\n",
        "    'facebook_audio2a': 5,\r\n",
        "    'facebook_audio2b': 5,\r\n",
        "    'facebook_audio3': 5,\r\n",
        "    'facebook_audio4': 5,\r\n",
        "    'hangouts_audio1a': 5,\r\n",
        "    'hangouts_audio1b': 5,\r\n",
        "    'hangouts_audio2a': 5,\r\n",
        "    'hangouts_audio2b': 5,\r\n",
        "    'hangouts_audio3': 5,\r\n",
        "    'hangouts_audio4': 5,\r\n",
        "    'skype_audio1a': 5,\r\n",
        "    'skype_audio1b': 5,\r\n",
        "    'skype_audio2a': 5,\r\n",
        "    'skype_audio2b': 5,\r\n",
        "    'skype_audio3': 5,\r\n",
        "    'skype_audio4': 5,\r\n",
        "    # VPN: Chat\r\n",
        "    'vpn_aim_chat1a': 6,\r\n",
        "    'vpn_aim_chat1b': 6,\r\n",
        "    'vpn_facebook_chat1a': 6,\r\n",
        "    'vpn_facebook_chat1b': 6,\r\n",
        "    'vpn_hangouts_chat1a': 6,\r\n",
        "    'vpn_hangouts_chat1b': 6,\r\n",
        "    'vpn_icq_chat1a': 6,\r\n",
        "    'vpn_icq_chat1b': 6,\r\n",
        "    'vpn_skype_chat1a': 6,\r\n",
        "    'vpn_skype_chat1b': 6,\r\n",
        "    # VPN: File Transfer\r\n",
        "    'vpn_ftps_a': 7,\r\n",
        "    'vpn_ftps_b': 7,\r\n",
        "    'vpn_sftp_a': 7,\r\n",
        "    'vpn_sftp_b': 7,\r\n",
        "    'vpn_skype_files1a': 7,\r\n",
        "    'vpn_skype_files1b': 7,\r\n",
        "    # VPN: Email\r\n",
        "    'vpn_email2a': 8,\r\n",
        "    'vpn_email2b': 8,\r\n",
        "    # VPN: Streaming\r\n",
        "    'vpn_vimeo_a': 9,\r\n",
        "    'vpn_vimeo_b': 9,\r\n",
        "    'vpn_youtube_a': 9,\r\n",
        "    # VPN: Torrent\r\n",
        "    'vpn_bittorrent': 10,\r\n",
        "    # VPN VoIP\r\n",
        "    'vpn_facebook_audio2': 11,\r\n",
        "    'vpn_hangouts_audio1': 11,\r\n",
        "    'vpn_hangouts_audio2': 11,\r\n",
        "    'vpn_skype_audio1': 11,\r\n",
        "    'vpn_skype_audio2': 11,\r\n",
        "}\r\n",
        "\r\n",
        "ID_TO_TRAFFIC = {\r\n",
        "    0: 'Chat',\r\n",
        "    1: 'Email',\r\n",
        "    2: 'File Transfer',\r\n",
        "    3: 'Streaming',\r\n",
        "    4: 'Torrent',\r\n",
        "    5: 'Voip',\r\n",
        "    6: 'VPN: Chat',\r\n",
        "    7: 'VPN: File Transfer',\r\n",
        "    8: 'VPN: Email',\r\n",
        "    9: 'VPN: Streaming',\r\n",
        "    10: 'VPN: Torrent',\r\n",
        "    11: 'VPN: Voip',\r\n",
        "}\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJJRor0mtfWk"
      },
      "source": [
        "#Packet Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku3jJynpgQgH"
      },
      "source": [
        "def read_pcap(path: Path):\r\n",
        "    packets = rdpcap(str(path))\r\n",
        "    return packets\r\n",
        "\r\n",
        "\r\n",
        "def should_omit_packet(packet):\r\n",
        "    # SYN, ACK or FIN flags set to 1 and no payload\r\n",
        "    if TCP in packet and (packet.flags & 0x13):\r\n",
        "        # not payload or contains only padding\r\n",
        "        layers = packet[TCP].payload.layers()\r\n",
        "        if not layers or (Padding in layers and len(layers) == 1):\r\n",
        "            return True\r\n",
        "\r\n",
        "    # DNS segment\r\n",
        "    if DNS in packet:\r\n",
        "        return True\r\n",
        "\r\n",
        "    return False\r\n",
        "\r\n",
        "\r\n",
        "def remove_ether_header(packet):\r\n",
        "    if Ether in packet:\r\n",
        "        return packet[Ether].payload\r\n",
        "    return packet\r\n",
        "\r\n",
        "\r\n",
        "def mask_ip(packet):\r\n",
        "    if IP in packet:\r\n",
        "        packet[IP].src = '0.0.0.0'\r\n",
        "        packet[IP].dst = '0.0.0.0'\r\n",
        "\r\n",
        "    return packet\r\n",
        "\r\n",
        "\r\n",
        "def pad_udp(packet):\r\n",
        "    if UDP in packet:\r\n",
        "        # get layers after udp\r\n",
        "        layer_after = packet[UDP].payload.copy()\r\n",
        "\r\n",
        "        # build a padding layer\r\n",
        "        pad = Padding()\r\n",
        "        pad.load = '\\x00' * 12\r\n",
        "\r\n",
        "        layer_before = packet.copy()\r\n",
        "        layer_before[UDP].remove_payload()\r\n",
        "        packet = layer_before / pad / layer_after\r\n",
        "\r\n",
        "        return packet\r\n",
        "\r\n",
        "    return packet\r\n",
        "\r\n",
        "\r\n",
        "def packet_to_sparse_array(packet, max_length=1500):\r\n",
        "    # Original implementation\r\n",
        "    print('raw packet\\n', raw(packet))\r\n",
        "    arr = np.frombuffer(raw(packet), dtype=np.uint8)[0: max_length] / 255  # divide each byte by 255\r\n",
        "    print('arr before padding\\n', arr)\r\n",
        "    if len(arr) < max_length:\r\n",
        "        pad_width = max_length - len(arr)\r\n",
        "        arr = np.pad(arr, pad_width=(0, pad_width), constant_values=0)  # padding the packet\r\n",
        "        print('arr after padding\\n', arr)\r\n",
        "\r\n",
        "    arr = sparse.csr_matrix(arr)\r\n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQBWBSFLpTQK"
      },
      "source": [
        "def transform_packet(packet):\r\n",
        "    if should_omit_packet(packet):\r\n",
        "        return None\r\n",
        "    print('packet before delete\\n', packet)\r\n",
        "    packet = remove_ether_header(packet)\r\n",
        "    packet = pad_udp(packet)\r\n",
        "    packet = mask_ip(packet)\r\n",
        "    print('packet after delete\\n', packet)\r\n",
        "\r\n",
        "    arr = packet_to_sparse_array(packet) # if original implementation (original packet length)\r\n",
        "\r\n",
        "    return arr\r\n",
        "\r\n",
        "\r\n",
        "def transform_pcap(path, output_path: Path = None, output_batch_size=10000):\r\n",
        "    if Path(str(output_path.absolute()) + '_SUCCESS').exists():\r\n",
        "        print(output_path, 'Done')\r\n",
        "        return\r\n",
        "\r\n",
        "    print('Processing', path)\r\n",
        "\r\n",
        "    rows = []\r\n",
        "    batch_index = 0\r\n",
        "    for i, packet in enumerate(read_pcap(path)):\r\n",
        "        arr = transform_packet(packet)\r\n",
        "        if arr is not None:\r\n",
        "            # get labels for app identification\r\n",
        "            # print(arr)\r\n",
        "            prefix = path.name.split('.')[0].lower()\r\n",
        "            app_label = PREFIX_TO_APP_ID.get(prefix)\r\n",
        "            traffic_label = PREFIX_TO_TRAFFIC_ID.get(prefix)\r\n",
        "            row = {\r\n",
        "                'app_label': app_label,\r\n",
        "                'traffic_label': traffic_label,\r\n",
        "                'feature': arr.todense().tolist()[0]\r\n",
        "            }\r\n",
        "            # print(row)\r\n",
        "            rows.append(row)\r\n",
        "\r\n",
        "        # write every batch_size packets, by default 10000\r\n",
        "        if rows and i > 0 and i % output_batch_size == 0:\r\n",
        "            part_output_path = Path(str(output_path.absolute()) + f'_part_{batch_index:04d}.csv')\r\n",
        "            df = pd.DataFrame(rows)\r\n",
        "            df.to_csv(part_output_path)\r\n",
        "            batch_index += 1\r\n",
        "            rows.clear()\r\n",
        "\r\n",
        "    # final write\r\n",
        "    if rows:\r\n",
        "        df = pd.DataFrame(rows)\r\n",
        "        part_output_path = Path(str(output_path.absolute()) + f'_part_{batch_index:04d}.csv')\r\n",
        "        df.to_csv(part_output_path)\r\n",
        "\r\n",
        "    # write success file\r\n",
        "    with Path(str(output_path.absolute()) + '_SUCCESS').open('w') as f:\r\n",
        "        f.write('')\r\n",
        "\r\n",
        "    print(output_path, 'Done')\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSJlOITurp-9"
      },
      "source": [
        "@click.command()\r\n",
        "@click.option('-s', '--source', help='path to the directory containing raw pcap files', required=True)\r\n",
        "@click.option('-t', '--target', help='path to the directory for persisting preprocessed files', required=True)\r\n",
        "def main(source, target):\r\n",
        "    data_dir_path = Path(source)\r\n",
        "    target_dir_path = Path(target)\r\n",
        "    target_dir_path.mkdir(parents=True, exist_ok=True)\r\n",
        "\r\n",
        "    for pcap_path in sorted(data_dir_path.iterdir()):\r\n",
        "        transform_pcap(pcap_path, target_dir_path / (pcap_path.name + '.transformed'))\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0naE4XOtr8g"
      },
      "source": [
        "#Load data after preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrADNL24txD8"
      },
      "source": [
        "def preprocessing_features(df_train):\r\n",
        "    first_drop = pd.DataFrame(df_train['feature'].str.split('['))\r\n",
        "    x = pd.DataFrame(first_drop.feature.values.tolist()).add_prefix('feature_')\r\n",
        "    x.drop('feature_0', axis=1, inplace=True)\r\n",
        "    second_drop = pd.DataFrame(x['feature_1'].str.split(']'))\r\n",
        "    x = pd.DataFrame(second_drop.feature_1.values.tolist()).add_prefix('feature_')\r\n",
        "    x.drop('feature_1', axis=1, inplace=True)\r\n",
        "    x = x.rename({'feature_0':'feature'}, axis=1)\r\n",
        "    features = pd.DataFrame(x['feature'].str.split(','))\r\n",
        "    x = pd.DataFrame(features.feature.values.tolist()).add_prefix('feature_')\r\n",
        "    x = x.fillna(0)\r\n",
        "    x = x.apply(np.float64)\r\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNpqkcOGt0bG"
      },
      "source": [
        "from torch.utils.data import Dataset\r\n",
        "\r\n",
        "class PcapDataset(Dataset):\r\n",
        "    def __init__(self, df_train):\r\n",
        "      x = preprocessing_features(df_train)\r\n",
        "      y = df_train['traffic_label']\r\n",
        "      #y = df_train['app_label']\r\n",
        "      y = y.apply(np.float64)\r\n",
        "\r\n",
        "      self.x_train = torch.tensor(x.values, dtype=torch.float64)\r\n",
        "      self.y_train = torch.tensor(y.values, dtype=torch.float64)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.y_train)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return self.x_train[idx], self.y_train[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0DXZgpct5in",
        "outputId": "58258acc-fd1a-4708-a652-b1362383a4a7"
      },
      "source": [
        "# get the data from csv\r\n",
        "\r\n",
        "csvfile = pd.read_csv('/content/drive/MyDrive/combine.csv')\r\n",
        "df = csvfile.sample(20000)\r\n",
        "\r\n",
        "df['split'] = np.random.randn(df.shape[0], 1)\r\n",
        "\r\n",
        "msk = np.random.rand(len(df)) <= 0.8\r\n",
        "\r\n",
        "train = df[msk]\r\n",
        "test = df[~msk]\r\n",
        "train.drop(columns=['split'], inplace=True)\r\n",
        "test.drop(columns=['split'], inplace=True)\r\n",
        "\r\n",
        "train.dropna(axis=0, how='any', inplace=True)\r\n",
        "test.dropna(axis=0, how='any', inplace=True)\r\n",
        "\r\n",
        "pcap_train = PcapDataset(train)\r\n",
        "train_loader = torch.utils.data.DataLoader(pcap_train, batch_size=25, shuffle=True, drop_last=True)\r\n",
        "pcap_test = PcapDataset(test)\r\n",
        "test_loader = torch.utils.data.DataLoader(pcap_test, batch_size=25, shuffle=False, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaDJetaMryY0"
      },
      "source": [
        "#CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bNVKv9lrxrj"
      },
      "source": [
        "class CNN(nn.Module):\r\n",
        "\r\n",
        "  def __init__(self,n_classes):\r\n",
        "      super(CNN, self).__init__()\r\n",
        "      self.conv1 = nn.Conv1d(1, 200, 5, 2, 0)\r\n",
        "      self.conv2 = nn.Conv1d(200, 100, 4, 1, 0)\r\n",
        "\r\n",
        "      self.pool = nn.AvgPool1d(2)\r\n",
        "      self.dropout = nn.Dropout(p=0.25)\r\n",
        "\r\n",
        "      self.fc1 = nn.Linear(18500, 200)\r\n",
        "      self.fc2 = nn.Linear(200, 100)\r\n",
        "      self.fc3 = nn.Linear(100, 50)\r\n",
        "      self.out = nn.Linear(50,n_classes)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "      x = F.avg_pool1d(F.relu(self.conv1(x)),2)\r\n",
        "      x = self.dropout(x)\r\n",
        "      x = F.avg_pool1d(F.relu(self.conv2(x)),2)\r\n",
        "      x = self.dropout(x)\r\n",
        "\r\n",
        "      x = torch.flatten(x, start_dim=1)\r\n",
        "      x = F.relu(self.fc1(x))\r\n",
        "      x = self.dropout(x)     \r\n",
        "      x = F.relu(self.fc2(x))\r\n",
        "      x = self.dropout(x)\r\n",
        "      x = F.relu(self.fc3(x))\r\n",
        "      x = self.dropout(x)\r\n",
        "\r\n",
        "      x = self.out(x)\r\n",
        "\r\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waPS2IeZPqOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "176231a5-cb95-4e69-9d78-9d436676303f"
      },
      "source": [
        "n_classes = 12 #traffic\r\n",
        "#n_classes = 17 #App\r\n",
        "model = CNN(n_classes).cuda()\r\n",
        "print(model)\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN(\n",
            "  (conv1): Conv1d(1, 200, kernel_size=(5,), stride=(2,))\n",
            "  (conv2): Conv1d(200, 100, kernel_size=(4,), stride=(1,))\n",
            "  (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            "  (fc1): Linear(in_features=18500, out_features=200, bias=True)\n",
            "  (fc2): Linear(in_features=200, out_features=100, bias=True)\n",
            "  (fc3): Linear(in_features=100, out_features=50, bias=True)\n",
            "  (out): Linear(in_features=50, out_features=12, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfzXNJXwsE2_"
      },
      "source": [
        "#Auto-encoder model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcAgfCY-sK9s"
      },
      "source": [
        "class StackedAutoEncoder(nn.Module):\r\n",
        "    def __init__(self, n_classes):\r\n",
        "        super(StackedAutoEncoder, self).__init__()\r\n",
        "    \r\n",
        "        # encoder\r\n",
        "        self.enc1 = nn.Linear(in_features=1500 , out_features=300)\r\n",
        "        self.enc2 = nn.Linear(in_features=300, out_features=60)\r\n",
        "\r\n",
        "        # decoder \r\n",
        "        self.dec1 = nn.Linear(in_features=60, out_features=300)\r\n",
        "        self.dec2 = nn.Linear(in_features=300, out_features=1500)\r\n",
        "\r\n",
        "        #out\r\n",
        "        self.fc_out = nn.Linear(1500, n_classes)\r\n",
        "        self.lsm = nn.Softmax(dim=0)\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = F.relu(self.enc1(x))\r\n",
        "        x = F.relu(self.enc2(x))\r\n",
        "  \r\n",
        "        x = F.relu(self.dec1(x))\r\n",
        "        x = F.relu(self.dec2(x))\r\n",
        "\r\n",
        "        x = self.fc_out(x)\r\n",
        "        \r\n",
        "        return x\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzrE1muaskeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "719296d6-fe97-4747-c480-ada1062c68e5"
      },
      "source": [
        "n_classes = 12 #traffic\r\n",
        "#n_classes = 17 #App\r\n",
        "model = StackedAutoEncoder(n_classes).cuda()\r\n",
        "print(model)\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "StackedAutoEncoder(\n",
            "  (enc1): Linear(in_features=1500, out_features=300, bias=True)\n",
            "  (enc2): Linear(in_features=300, out_features=60, bias=True)\n",
            "  (dec1): Linear(in_features=60, out_features=300, bias=True)\n",
            "  (dec2): Linear(in_features=300, out_features=1500, bias=True)\n",
            "  (fc_out): Linear(in_features=1500, out_features=12, bias=True)\n",
            "  (lsm): Softmax(dim=0)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "437mnFzxsy8P"
      },
      "source": [
        "#training CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUciL9_cs09g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73dfe17c-36ae-4d4e-9e4f-bfefc7a89865"
      },
      "source": [
        "# Train the model\r\n",
        "train_loss = []\r\n",
        "test_loss = []\r\n",
        "interval_tuples = []\r\n",
        "start = time.time()\r\n",
        "\r\n",
        "for epoch in range(5):\r\n",
        "    running_train_loss = 0.0\r\n",
        "    for i, data in enumerate(train_loader, 0):\r\n",
        "        inputs, labels = data\r\n",
        "        labels = labels.long()\r\n",
        "        inputs = np.expand_dims(inputs, axis=1)\r\n",
        "        inputs = torch.Tensor(inputs)\r\n",
        "        inputs = inputs.cuda() # -- for GPU\r\n",
        "        labels = labels.cuda() # -- for GPU\r\n",
        "        \r\n",
        "        # zero the parameters gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # forward + backward + optimization\r\n",
        "        outputs = model(inputs)\r\n",
        "\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        running_train_loss += loss.item()\r\n",
        "        if (i+1) % 100 == 0:\r\n",
        "            interval_tuples.append(str((epoch + 1, i + 1)))\r\n",
        "            print(\"[{}, {}] loss: {}\".format(epoch + 1, i + 1, running_train_loss / 100))\r\n",
        "            train_loss.append(running_train_loss / 100)\r\n",
        "            running_train_loss = 0.0\r\n",
        "\r\n",
        "stop = time.time()\r\n",
        "print(\"Training time: {}\".format(stop-start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 100] loss: 1.590349280834198\n",
            "[1, 200] loss: 1.237583685517311\n",
            "[2, 100] loss: 1.034083986878395\n",
            "[2, 200] loss: 0.8310993477702141\n",
            "[3, 100] loss: 0.5211205231398344\n",
            "[3, 200] loss: 0.49101036347448823\n",
            "[4, 100] loss: 0.36555207557976244\n",
            "[4, 200] loss: 0.33150065086781977\n",
            "[5, 100] loss: 0.2747922194376588\n",
            "[5, 200] loss: 0.2598778015188873\n",
            "Training time: 374.77481293678284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGnuSxRZwWZ9"
      },
      "source": [
        "#training autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C__t9Yx6wTmM",
        "outputId": "fb295ac4-4d27-4ce3-d34e-2eda6a5540a3"
      },
      "source": [
        "# Train the model\r\n",
        "train_loss = []\r\n",
        "test_loss = []\r\n",
        "interval_tuples = []\r\n",
        "start = time.time()\r\n",
        "\r\n",
        "for epoch in range(5):\r\n",
        "    running_train_loss = 0.0\r\n",
        "    for i, data in enumerate(train_loader, 0):\r\n",
        "        inputs, labels = data\r\n",
        "        labels = labels.long()\r\n",
        "        inputs = inputs.float()\r\n",
        "        inputs = torch.Tensor(inputs)\r\n",
        "        inputs = inputs.cuda() # -- for GPU\r\n",
        "        labels = labels.cuda() # -- for GPU\r\n",
        "        \r\n",
        "        # zero the parameters gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        outputs = model(inputs)\r\n",
        "\r\n",
        "        outputs\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        # print statistics \r\n",
        "        running_train_loss += loss.item()\r\n",
        "        if (i+1) % 100 == 0:\r\n",
        "            interval_tuples.append(str((epoch + 1, i + 1)))\r\n",
        "            print(\"[{}, {}] loss: {}\".format(epoch + 1, i + 1, running_train_loss / 100))\r\n",
        "            train_loss.append(running_train_loss / 100)\r\n",
        "            running_train_loss = 0.0\r\n",
        "\r\n",
        "stop = time.time()\r\n",
        "print(\"Training time: {}\".format(stop-start))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 100] loss: 1.1530652749538421\n",
            "[1, 200] loss: 0.6814604786038398\n",
            "[2, 100] loss: 0.5073436924815178\n",
            "[2, 200] loss: 0.47935782119631765\n",
            "[3, 100] loss: 0.2891769541427493\n",
            "[3, 200] loss: 0.26466762710362673\n",
            "[4, 100] loss: 0.1631641717813909\n",
            "[4, 200] loss: 0.1942844560649246\n",
            "[5, 100] loss: 0.1297033405629918\n",
            "[5, 200] loss: 0.11124686865136027\n",
            "Training time: 24.672285795211792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8tyCnp-SgAT"
      },
      "source": [
        "#CNN Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZX5qkiEP06R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "628a5689-977e-4175-dd9d-58b46e3e3a10"
      },
      "source": [
        "total = 0\r\n",
        "correct = 0\r\n",
        "with torch.no_grad(): \r\n",
        "    for data in test_loader:\r\n",
        "        inputs, labels = data\r\n",
        "        labels = labels.float()\r\n",
        "        inputs = np.expand_dims(inputs, axis=1)\r\n",
        "        inputs = torch.Tensor(inputs)\r\n",
        "        inputs = inputs.cuda() # -- for GPU\r\n",
        "        labels = labels.cuda() # -- for GPU\r\n",
        "\r\n",
        "        outputs = model(inputs)\r\n",
        "        _, predicted = torch.max(outputs.data, 1)\r\n",
        "        total += labels.size(0)\r\n",
        "        correct += (predicted == labels).sum().item()\r\n",
        "print('Accuracy: {}%'.format((100 * correct / total)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 92.52173913043478%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKnmT_ID8YGk"
      },
      "source": [
        "#Autoencoder Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZPSui318O9H",
        "outputId": "800cf574-5a2e-4213-c7c7-382cb1260be8"
      },
      "source": [
        "total = 0\r\n",
        "correct = 0\r\n",
        "with torch.no_grad(): \r\n",
        "    for data in test_loader:\r\n",
        "        inputs, labels = data\r\n",
        "        labels = labels.float()\r\n",
        "        #inputs = np.expand_dims(inputs, axis=1)\r\n",
        "        inputs = inputs.float()\r\n",
        "        inputs = torch.Tensor(inputs)\r\n",
        "        inputs = inputs.cuda() # -- for GPU\r\n",
        "        labels = labels.cuda() # -- for GPU\r\n",
        "\r\n",
        "        outputs = model(inputs)\r\n",
        "        _, predicted = torch.max(outputs.data, 1)\r\n",
        "        total += labels.size(0)\r\n",
        "        correct += (predicted == labels).sum().item()\r\n",
        "print('Accuracy: {}%'.format((100 * correct / total)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 93.97101449275362%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}